------------------------------------------------------------
| CHAPTER 2 - WRITING TEST FUNCTIONS                       |
------------------------------------------------------------

- This is the file structure for our Tasks project:

    - tasks_proj/
        - CHANGELOG.rst
        - LICENSE
        - MANIFEST.in
        - README.rst
        - setup.py
        - src/
            - tasks/
                - __init__.py
                - api.py
                - cli.py
                - config.py
                - tasksdb_pymongo.py
                - tasksdb_tinydb.py
        - tests
            - conftest.py
            - pytest.ini
            - func
                - __init__.py
                - test_add.py
                - ...
            - unit
                - __init__.py
                - test_task.py
                - ...



- Uses of files

    - 'src/tasks/__init__.py' file tells Python that the directory is a package.  It also acts as
        the main interface to the package when someone uses 'import tasks' (it allows a user to 
        call tasks.add() instead of tasks.api.add()).

    - 'tests/func/__init__.py' and 'tests/unit/__itit__.py' are empty.  They tell pytest to go up one
        directory to look for the root of the test directory and the pytest.ini file.

    - 'pytest.ini' is optional.  It contains project-wide pytest configuration.

    - 'conftest.py' is also optional.  It is used as a 'local plugin' by pytest, and can contain 
        hook functions and fixtures.



- Installing a Module For Testing

    - The easiest way to make a module available for testing is to install it with pip.  The -e flag makes 
        it possible to edit the source code of the module while also working on the tests.

        $ cd tasks_proj/..
        $ pip install -e tasks_proj

        # Now, we can run tests
        pytest test_task.py



- Using 'assert' Statements

    Pytest uses the simple 'assert' statement for everything, in contrast to other libraries that
      include various assertion helpers.

      pytest                  unittest
      ----------------------------------
      assert something        assertTrue(something)
      assert a == b           assertEqual(a, b)
      assert a <= b           assertLessEqual(a, b)


    The assert statement can be used with any expression.  If the boolean conversion of the 
      expression evaluates to True, the test passes.  If it evaulates to False, the test fails.



- Expecting Exceptions

    # The 'raises' statement catches an expected exception
    import pytest

    def test_add_raises():
        with pytest.raises(TypeError):
            tasks.add(task='not a Task object')



- Marking Test Functions

    # Mark a test with an attribute so it can be run in a subset
    @pytest.mark.list
    @pytest.mark.smoke
    def test list_raises():
        """list() should raise an exception with wrong type param."""
        with pytest.raises(TypeError):
            tasks.list_tasks(owner=123)


    $ pytest -v -m 'smoke' test_api_exceptions.py
    $ pytest -v -m 'smoke and list' test_api_exceptions.py
    $ pytest -v -m 'somke and not list' test_api_exceptions.py



- Creating Fixtures

    # Define a fixture that will run before and after every test in this file
    @pytest.fixture(autouse=True)
    def initialized_tasks_db(tmpdir):
        # Setup: start db
        tasks.start_tasks_db(str(tempdir), 'tiny')

        yield

        # Teardown: stop db
        tasks.stop_tasks_db()



- Skipping Tests

    # Skip a test
    @pytest.mark.skip(reason='misunderstood the api')
    def test_unique_id_1():
        ...


    # Conditionally skip a test
    @pytest.mark.skipif(tasks.__version__ < '0.2.0',
                        reason='not supported until version 0.2.0')
    def test_unique_id_2():
        ...



- Expecting Failed Tests

    # Mark a test as expected to fail
    @pytest.mark.xfail(tasks.__version__ < '0.2.0',
                       reason='not supported until version 0.2.0')
    def test_unique_id_1():
        ...


    @pytest.xfail()
    def test_unique_id_2():
        ...


    $ pytest test_unique_ids.py
    ========== 1 passed, 2 xfailed, 1 xpassed in 0.07 seconds

    'xfailed' means a test that was expected to fail
    'xpassed' means a test that was expceted to fail, but passed



- Parameterized Testing

    # Run the same task with a few different inputs
    @pytest.mark.parameterize('task',
                              [Task('sleep', done=True),
                               Task('wake', 'brian'),
                               Task('breathe', 'BRIAN', True),
                               Task['exercise', 'BrIaN', False]])
    def test_add(task):
        task_id = tasks.add(task)
        t_from_db = tasks.get(task_id)
        assert equivalent(t_from_db, task)


    $ pytest test_add.py::test_add
    test_add.py::test_add[task0] PASSED
    test_add.py::test_add[task1] PASSED
    test_add.py::test_add[task2] PASSED
    test_add.py::test_add[task3] PASSED
    =================== 4 passed in 0.05 seconds


    # Note that the 'equivalent()' statement checks to see if 2 objects have the all the same 
    #   values, except for 'id'.


    # 'parameterize' with multiple parameters
    @pytest.mark.parameterize('summary, owner, done',
                              [(​'sleep'​, None, False),
                               (​'wake'​, ​'brian'​, False),
                               (​'breathe'​, ​'BRIAN'​, True),
                               (​'eat eggs'​, ​'BrIaN'​, False)])
    def test_add(summary, owner, done):
        task = Task(summary, owner, done)
        task_id = tasks.add(task)
        t_from_db = tasks.get(task_id)
        assert equivalent(t_from_db, task)